Machine Learning & Deep Learning Projects

Welcome to my repository of Machine Learning (ML) and Deep Learning (DL) projects. This collection showcases my journey through key ML/DL concepts, ranging from foundational algorithms to modern neural architectures, including a basic exploration of Small Language Models (SLMs) and GPT-like structures.

Topics Covered:

Linear Regression

Decision Trees

Traditional Neural Networks

Small Language Models (SLMs) — inspired by Andrej Karpathy's work

Large Language Models (LLMs) — preprocessing, architecture, and pretraining pipelines

Project Highlights:

Linear Regression & Decision Trees:
Implemented from scratch and using libraries like Scikit-learn. These projects laid the groundwork for understanding supervised learning and model evaluation metrics.

Neural Networks:
Basic feedforward networks trained on classification tasks using PyTorch. Emphasis on architecture, activation functions, and backpropagation.

Small Language Models (SLMs):
A simple character-level language model inspired by Andrej Karpathy’s nanoGPT.

Dataset: Trained on the Tiny Stories dataset — a curated collection of short children’s stories specifically designed to train compact language models.

Note: Due to computational limitations, full training wasn't feasible, and the output may not be semantically meaningful.

Large Language Models (LLMs):
Includes detailed typed notes on the GPT architecture, covering:

Tokenization

Positional Embeddings

Attention Mechanism

Transformer Blocks

Pretraining Objectives

Self-supervised Learning

These notes are provided in the attached document for deeper understanding.

Skills Gained:

Strong coding practices in Python & PyTorch

Data preprocessing & model training pipelines

Deep theoretical understanding of transformer models

Insight into scaling challenges of language models
